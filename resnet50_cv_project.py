# -*- coding: utf-8 -*-
"""Final_‡∏Æ‡∏±‡∏•‡πÇ‡∏´‡∏•‡πÇ‡∏ö‡πã‡∏ß ‡πÇ‡∏Å‡πã‡∏°‡∏≤‡πÅ‡∏£‡πã‡∏ß‡∏ß_Resnet50_‡πÄ‡∏ß‡∏≠‡∏£‡πå‡∏ä‡∏±‡πà‡∏ô‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á4_CV_project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1gntZN0RvPviF3cu1P8o7r6o6OnP89JrR

# Import
"""

from google.colab import drive
drive.mount('/content/drive')

import zipfile
import json
import os
import shutil
import scipy.io
from collections import Counter
import pandas as pd
import matplotlib.pyplot as plt
import random
from tqdm import tqdm
import numpy as np
import matplotlib.pyplot as plt
import itertools
import sklearn
from sklearn.metrics import classification_report, confusion_matrix
import seaborn as sns
import cv2
from collections import Counter
from tensorflow.keras import mixed_precision
import json
import pickle

from tensorflow import keras
import tensorflow as tf
from tensorflow.compat.v1 import ConfigProto, InteractiveSession
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.applications.resnet50 import preprocess_input
from tensorflow.keras import layers
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint, CSVLogger

mixed_precision.set_global_policy('mixed_float16')

"""# ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏° Dataset"""

zip_path = "/content/drive/MyDrive/Datasets/stanford_car_dataset_by_classes.zip"
extract_path = "/content/Dataset"

# ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå‡∏ñ‡πâ‡∏≤‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏°‡∏µ
os.makedirs(extract_path, exist_ok=True)

# ‡πÅ‡∏ï‡∏Å‡πÑ‡∏ü‡∏•‡πå
with zipfile.ZipFile(zip_path, 'r') as zip_ref:
    zip_ref.extractall(extract_path)

DATASET_DIR = "/content/Dataset"

TRAIN_DIR = os.path.join(DATASET_DIR, "car_data", "car_data", "train")
TEST_DIR = os.path.join(DATASET_DIR, "car_data", "car_data", "test")

# ‡∏ô‡∏±‡∏ö‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏†‡∏≤‡∏û‡πÉ‡∏ô‡πÅ‡∏ï‡πà‡∏•‡∏∞‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå
train_counts = Counter()
for cls in os.listdir(TRAIN_DIR):
    cls_path = os.path.join(TRAIN_DIR, cls)
    if os.path.isdir(cls_path):
        n_images = len([f for f in os.listdir(cls_path) if f.lower().endswith(('.jpg', '.png', '.jpeg'))])
        train_counts[cls] = n_images

# ‡∏™‡∏£‡πâ‡∏≤‡∏á DataFrame
train_df = pd.DataFrame(train_counts.items(), columns=["Class", "Train Images"])
train_df = train_df.sort_values("Train Images", ascending=False)

# ‡πÅ‡∏™‡∏î‡∏á‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á
print("üìä ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏†‡∏≤‡∏û‡πÉ‡∏ô train:")
print(train_df.head(10))

# ‡∏û‡∏•‡πá‡∏≠‡∏ï‡∏Å‡∏£‡∏≤‡∏ü
plt.figure(figsize=(12, 5))
plt.bar(train_df["Class"], train_df["Train Images"], color="steelblue", label="Train")
plt.title("Distribution of images per class (Train set)")
plt.xlabel("Car Class")
plt.ylabel("Number of Images")
plt.xticks(rotation=90, fontsize=6)
plt.legend()
plt.tight_layout()
plt.show()

print("\n‡∏£‡∏ß‡∏°‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î:")
print(f"Train: {train_df['Train Images'].sum()} ‡∏£‡∏π‡∏õ")
print(f"‡∏£‡∏ß‡∏°‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î {len(train_df)} ‡∏Ñ‡∏•‡∏≤‡∏™")

ROOT_DIR = "/content/Dataset/car_data/car_data/train"
OUTPUT_DIR = "/content/FinalDataset"
TRAIN_OUT = os.path.join(OUTPUT_DIR, "train")
VAL_OUT = os.path.join(OUTPUT_DIR, "val")

TRAIN_RATIO = 0.75
VAL_RATIO = 0.25

random.seed(42)  # reproducible

# ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå‡∏õ‡∏•‡∏≤‡∏¢‡∏ó‡∏≤‡∏á
os.makedirs(TRAIN_OUT, exist_ok=True)
os.makedirs(VAL_OUT, exist_ok=True)

# ‡∏ß‡∏ô‡∏ó‡∏∏‡∏Å class
for class_name in tqdm(os.listdir(ROOT_DIR), desc="Splitting"):
    class_path = os.path.join(ROOT_DIR, class_name)
    if not os.path.isdir(class_path):
        continue

    images = [f for f in os.listdir(class_path) if f.lower().endswith(('.jpg', '.jpeg', '.png'))]
    if len(images) == 0:
        continue

    random.shuffle(images)

    # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏†‡∏≤‡∏û‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏ä‡∏∏‡∏î
    n_total = len(images)
    n_train = int(round(n_total * TRAIN_RATIO))
    n_val = n_total - n_train  # ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡∏£‡∏ß‡∏°‡∏Å‡∏±‡∏ô‡∏Ñ‡∏£‡∏ö‡∏û‡∏≠‡∏î‡∏µ

    train_imgs = images[:n_train]
    val_imgs   = images[n_train:]

    # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå class ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏ä‡∏∏‡∏î
    os.makedirs(os.path.join(TRAIN_OUT, class_name), exist_ok=True)
    os.makedirs(os.path.join(VAL_OUT, class_name), exist_ok=True)

    # copy ‡πÑ‡∏õ‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏ä‡∏∏‡∏î
    for img in train_imgs:
        shutil.copy2(os.path.join(class_path, img),
                     os.path.join(TRAIN_OUT, class_name, img))

    for img in val_imgs:
        shutil.copy2(os.path.join(class_path, img),
                     os.path.join(VAL_OUT, class_name, img))

def count_imgs(path):
    total = 0
    for cls in os.listdir(path):
        cls_dir = os.path.join(path, cls)
        if os.path.isdir(cls_dir):
            total += len(os.listdir(cls_dir))
    return total

print("Train images:", count_imgs(TRAIN_OUT))
print("Val images:  ", count_imgs(VAL_OUT))
print("Test images:  ", count_imgs(TEST_DIR))

"""# Train"""

print("GPU:", tf.config.list_physical_devices('GPU'))

# ‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô TensorFlow ‡∏à‡∏≠‡∏á VRAM ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î
gpus = tf.config.experimental.list_physical_devices('GPU')
for gpu in gpus:
    tf.config.experimental.set_memory_growth(gpu, True)

BASE_DIR = '/content/drive/MyDrive/Model/FinalResnet_Final'
HISTORY_DIR = os.path.join(BASE_DIR, 'history')
MODEL_DIR = os.path.join(BASE_DIR, 'models')
LOG_DIR = os.path.join(BASE_DIR, 'logs')

for folder in [BASE_DIR, HISTORY_DIR, MODEL_DIR, LOG_DIR]:
    if not os.path.exists(folder):
        os.makedirs(folder)
        print(f"‚úÖ Created: {folder}")

TRAIN_DIR = "/content/FinalDataset/train"
VAL_DIR   = "/content/FinalDataset/val"
TEST_DIR  = "/content/Dataset/car_data/car_data/test"

IMG_SIZE = 448
BATCH_SIZE = 16
NUM_CLASSES = 196
SEED = 42

def set_global_seed(seed):
    os.environ['PYTHONHASHSEED'] = str(seed)
    random.seed(seed)
    np.random.seed(seed)
    tf.random.set_seed(seed)
    tf.keras.utils.set_random_seed(seed)

set_global_seed(SEED)

train_ds = tf.keras.utils.image_dataset_from_directory(
    TRAIN_DIR,
    image_size=(IMG_SIZE, IMG_SIZE),
    batch_size=BATCH_SIZE,
    shuffle=True,
    label_mode='categorical',
    seed=SEED
)

val_ds = tf.keras.utils.image_dataset_from_directory(
    VAL_DIR,
    image_size=(IMG_SIZE, IMG_SIZE),
    batch_size=BATCH_SIZE,
    shuffle=False,
    label_mode='categorical'
)

test_ds = tf.keras.utils.image_dataset_from_directory(
    TEST_DIR,
    image_size=(IMG_SIZE, IMG_SIZE),
    batch_size=BATCH_SIZE,
    shuffle=False,
    label_mode='categorical'
)

class_names = train_ds.class_names
with open(os.path.join(BASE_DIR, 'class_names.json'), 'w') as f:
    json.dump(class_names, f)
print(f"‚úÖ Saved {len(class_names)} classes.")

"""## Augmentation"""

data_augmentation = keras.Sequential([
    layers.RandomFlip("horizontal"),
    layers.RandomRotation(0.1),
    layers.RandomZoom(0.1),
    layers.RandomTranslation(0.1,0.1),
    layers.RandomContrast(0.2),    # ‡πÅ‡∏Å‡πâ‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏à‡∏≥‡∏™‡∏µ
    layers.RandomBrightness(0.2),  # ‡πÅ‡∏Å‡πâ‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏à‡∏≥‡πÅ‡∏™‡∏á
])

train_ds = train_ds.prefetch(buffer_size=tf.data.AUTOTUNE)
val_ds = val_ds.prefetch(buffer_size=tf.data.AUTOTUNE)
test_ds = test_ds.prefetch(buffer_size=tf.data.AUTOTUNE)

"""## Model Building"""

base_model = tf.keras.applications.ResNet50(
    input_shape=(IMG_SIZE, IMG_SIZE, 3),
    include_top=False,
    weights='imagenet'
)

base_model.trainable = False # Freeze ‡πÑ‡∏ß‡πâ‡∏Å‡πà‡∏≠‡∏ô

inputs = tf.keras.Input(shape=(IMG_SIZE, IMG_SIZE, 3))
x = data_augmentation(inputs)
x = tf.keras.applications.resnet50.preprocess_input(x)

x = base_model(inputs, training=False)

x = tf.keras.layers.GlobalAveragePooling2D()(x)
x = tf.keras.layers.BatchNormalization()(x)
x = tf.keras.layers.Dropout(0.5)(x)

x = tf.keras.layers.Dense(512, activation='relu')(x)
x = tf.keras.layers.BatchNormalization()(x)
x = tf.keras.layers.Dropout(0.5)(x)

outputs = tf.keras.layers.Dense(NUM_CLASSES, activation='softmax',dtype='float32')(x)

model = tf.keras.Model(inputs, outputs)

def save_history_pickle(history, filename):
    path = os.path.join(HISTORY_DIR, filename)
    with open(path, 'wb') as f:
        pickle.dump(history.history, f)
    print(f"üíæ Pickle History saved: {path}")

optimizer_head = tf.keras.optimizers.Adam(learning_rate=1e-3)

model.compile(optimizer=optimizer_head,
              loss=tf.keras.losses.CategoricalCrossentropy(label_smoothing=0.1),
              metrics=['accuracy'])

model.summary()

callbacks_phase1 = [
    EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True, verbose=1),
    ModelCheckpoint(os.path.join(MODEL_DIR, 'resnet_phase1_head.keras'),
                    save_best_only=True, monitor='val_accuracy', mode='max', verbose=1),
    CSVLogger(os.path.join(LOG_DIR, 'log_phase1.csv'), separator=',', append=True)
]

callbacks_phase2 = [
    EarlyStopping(monitor='val_loss', patience=8, restore_best_weights=True, verbose=1),
    ModelCheckpoint(os.path.join(MODEL_DIR, 'resnet_best_final.keras'),
                    save_best_only=True, monitor='val_accuracy', mode='max', verbose=1),
    ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, min_lr=1e-5, verbose=1),
    CSVLogger(os.path.join(LOG_DIR, 'log_phase2.csv'), separator=',', append=True)
]

"""## Phase1"""

# Phase 1: Train Head
history1 = model.fit(
    train_ds,
    validation_data=val_ds,
    epochs=10,
    callbacks=callbacks_phase1
)

save_history_pickle(history1, 'history_phase1.pkl')

acc = history1.history['accuracy']
val_acc = history1.history['val_accuracy']

loss = history1.history['loss']
val_loss = history1.history['val_loss']

plt.figure(figsize=(8, 8))
plt.subplot(2, 1, 1)
plt.plot(acc, label='Training Accuracy')
plt.plot(val_acc, label='Validation Accuracy')
plt.legend(loc='lower right')
plt.ylabel('Accuracy')
plt.ylim([min(plt.ylim()),1])
plt.title('Training and Validation Accuracy')

plt.subplot(2, 1, 2)
plt.plot(loss, label='Training Loss')
plt.plot(val_loss, label='Validation Loss')
plt.legend(loc='upper right')
plt.ylabel('Cross Entropy')
plt.ylim([0,10.0])
plt.title('Training and Validation Loss')
plt.xlabel('epoch')
plt.show()

checkpoint_path = '/content/drive/MyDrive/Model/FinalResnet_Final/resnet_phase1_head.keras'

# ‡πÇ‡∏´‡∏•‡∏î‡∏ô‡πâ‡∏≥‡∏´‡∏ô‡∏±‡∏Å (Weights) ‡∏Å‡∏•‡∏±‡∏ö‡∏°‡∏≤
print(f"‚è≥ Loading weights from {checkpoint_path}...")
try:
    model.load_weights(checkpoint_path)
    print("Load ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à!")
except Exception as e:
    print(f"Load ‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ: {e}")

"""## Phase2"""

base_model.trainable = True

optimizer_tune = tf.keras.optimizers.SGD(learning_rate=0.005, momentum=0.9, decay=0.0001)

model.compile(optimizer=optimizer_tune,
              loss=tf.keras.losses.CategoricalCrossentropy(label_smoothing=0.1),
              metrics=['accuracy'])

model.summary()

history2 = model.fit(
    train_ds,
    validation_data=val_ds,
    epochs=35,
    callbacks=callbacks_phase2
)

save_history_pickle(history2, 'history_phase2.pkl')

"""## Phase3"""

checkpoint_path = '/content/drive/MyDrive/Model/FinalResnet_Final/models/resnet_best_final.keras'
print(f"‚è≥ Resuming from checkpoint: {checkpoint_path}")
try:
    model.load_weights(checkpoint_path)
    print("Load ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à")
except Exception as e:
    print(f"Load ‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ: {e}")

base_model.trainable = True

optimizer_fine = tf.keras.optimizers.SGD(learning_rate=0.0005, momentum=0.9, decay=0.0001)

model.compile(optimizer=optimizer_fine,
              loss=tf.keras.losses.CategoricalCrossentropy(label_smoothing=0.1),
              metrics=['accuracy'])

callbacks_final = [
    tf.keras.callbacks.EarlyStopping(patience=8, restore_best_weights=True, verbose=1),
    tf.keras.callbacks.ModelCheckpoint(
        filepath='/content/drive/MyDrive/Model/FinalResnet_Final/models/resnet50_final_final_final.keras',
        save_best_only=True, monitor='val_accuracy', mode='max', verbose=1
    ),
    tf.keras.callbacks.ReduceLROnPlateau(factor=0.2, patience=2, min_lr=1e-6),
    tf.keras.callbacks.CSVLogger(
        os.path.join(LOG_DIR, 'log_phase3.csv'),
        separator=',', append=True
    )
]

history_final = model.fit(
    train_ds,
    validation_data=val_ds,
    initial_epoch=35,
    epochs=50,
    callbacks=callbacks_final
)

save_history_pickle(history_final, 'history_phase3.pkl')

"""## Model sanity check"""

# ‡πÄ‡∏ä‡πá‡∏Å‡∏ß‡πà‡∏≤‡πÇ‡∏°‡πÄ‡∏î‡∏• output shape ‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ö‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏Ñ‡∏•‡∏≤‡∏™
x_sample = tf.random.normal((1, IMG_SIZE, IMG_SIZE, 3))
y_pred = model(x_sample)
print("‚úÖ Output shape:", y_pred.shape)
print("‚úÖ Sum of softmax probabilities:", tf.reduce_sum(y_pred).numpy())

assert y_pred.shape[-1] == NUM_CLASSES, "‚ùå Output class ‡πÑ‡∏°‡πà‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ö‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏Ñ‡∏•‡∏≤‡∏™!"
print("‚úÖ Number of output classes matches:", NUM_CLASSES)

# ‡∏î‡∏π‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• batch ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏à‡∏≤‡∏Å train_ds
images, labels = next(iter(train_ds))

print("‚úÖ Image batch shape:", images.shape)
print("‚úÖ Label batch shape:", labels.shape)

# ‡∏î‡∏π‡∏ä‡∏∑‡πà‡∏≠‡∏Ñ‡∏•‡∏≤‡∏™‡∏à‡∏≤‡∏Å directory
print(f"‡∏°‡∏µ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î {len(class_names)} classes ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÄ‡∏ä‡πà‡∏ô:", class_names[:10])

# ‡πÅ‡∏™‡∏î‡∏á‡∏†‡∏≤‡∏û‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á 9 ‡∏£‡∏π‡∏õ ‡∏û‡∏£‡πâ‡∏≠‡∏° label ‡∏ó‡∏µ‡πà‡∏ï‡∏£‡∏á‡∏à‡∏≤‡∏Å class_names
plt.figure(figsize=(10, 10))
for i in range(9):
    ax = plt.subplot(3, 3, i + 1)
    plt.imshow(images[i].numpy().astype("uint8"))
    label_idx = np.argmax(labels[i].numpy())
    plt.title(class_names[label_idx])
    plt.axis("off")
plt.show()

# ‡∏•‡∏≠‡∏á‡πÉ‡∏´‡πâ‡πÇ‡∏°‡πÄ‡∏î‡∏• predict ‡∏†‡∏≤‡∏û batch ‡πÅ‡∏£‡∏Å (‡πÄ‡∏ä‡πá‡∏Å‡∏ß‡πà‡∏≤‡πÑ‡∏°‡πà crash ‡πÅ‡∏•‡∏∞‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏°‡∏±‡πà‡∏ô‡πÉ‡∏à)
preds = model.predict(images[:5])

for i in range(5):
    plt.imshow(images[i].numpy().astype("uint8"))
    plt.axis("off")
    true_label = class_names[np.argmax(labels[i])]
    pred_label = class_names[np.argmax(preds[i])]
    confidence = np.max(preds[i])
    color = "green" if true_label == pred_label else "red"
    plt.title(f"Pred: {pred_label}\nTrue: {true_label}\nConf: {confidence:.2f}", color=color)
    plt.show()

# ‡∏ï‡∏£‡∏ß‡∏à‡∏Ñ‡πà‡∏≤ NaN ‡∏´‡∏£‡∏∑‡∏≠ infinity ‡πÉ‡∏ô weight ‡∏Ç‡∏≠‡∏á‡πÇ‡∏°‡πÄ‡∏î‡∏•
for layer in model.layers:
    for weight in layer.weights:
        w = weight.numpy()
        if np.isnan(w).any() or np.isinf(w).any():
            print(f"‚ö†Ô∏è ‡∏û‡∏ö NaN/Inf ‡πÉ‡∏ô layer: {layer.name}")
print("‚úÖ ‡πÑ‡∏°‡πà‡∏°‡∏µ NaN/Inf ‡πÉ‡∏ô‡πÇ‡∏°‡πÄ‡∏î‡∏•")

"""# Save Model"""

save_path = '/content/drive/MyDrive/Model/FinalResnet_Final/models/resnet50_final.keras'

# 1. ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå‡∏Å‡πà‡∏≠‡∏ô‡πÄ‡∏™‡∏°‡∏≠ (‡∏Å‡∏±‡∏ô Error)
os.makedirs(os.path.dirname(save_path), exist_ok=True)

# 2. ‡πÄ‡∏ã‡∏ü‡πÇ‡∏°‡πÄ‡∏î‡∏•
model.save(save_path)
print(f"Model saved successfully at: {save_path}")

"""# Evaluate"""

# Evaluate ‡∏ö‡∏ô Test Set
test_loss, test_acc = model.evaluate(test_ds)
print(f"‚úÖ Test Accuracy: {test_acc:.4f}")
print(f"‚úÖ Test Loss: {test_loss:.4f}")

best_model_path = os.path.join(MODEL_DIR, 'resnet50_final_final_final.keras')
print(f"‚è≥ Loading best model: {best_model_path}")
final_model = tf.keras.models.load_model(best_model_path)

pool_images = []
pool_labels = []

for i, (batch_imgs, batch_lbls) in enumerate(test_ds):
    if i % 3 == 0: # ‡∏Å‡∏£‡∏∞‡πÇ‡∏î‡∏î‡∏Ç‡πâ‡∏≤‡∏° Batch ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡πÑ‡∏î‡πâ‡∏£‡∏ñ‡∏´‡∏•‡∏≤‡∏Å‡∏´‡∏•‡∏≤‡∏¢‡∏£‡∏∏‡πà‡∏ô
        # ‡∏™‡∏∏‡πà‡∏°‡∏´‡∏¢‡∏¥‡∏ö 1 ‡∏£‡∏π‡∏õ‡∏à‡∏≤‡∏Å‡πÉ‡∏ô Batch ‡∏ô‡∏µ‡πâ
        rand_idx = random.randint(0, len(batch_imgs) - 1)
        pool_images.append(batch_imgs[rand_idx])
        pool_labels.append(batch_lbls[rand_idx])

    # ‡∏û‡∏≠‡πÑ‡∏î‡πâ‡∏Ñ‡∏£‡∏ö‡∏™‡∏±‡∏Å 20-30 ‡∏£‡∏π‡∏õ‡∏Å‡πá‡∏û‡∏≠‡πÅ‡∏•‡πâ‡∏ß (‡πÄ‡∏¢‡∏≠‡∏∞‡πÑ‡∏õ‡πÄ‡∏î‡∏µ‡πã‡∏¢‡∏ß‡∏ä‡πâ‡∏≤)
    if len(pool_images) >= 20:
        break

indices = random.sample(range(len(pool_images)), min(10, len(pool_images)))

plt.figure(figsize=(20, 8))

for i, idx in enumerate(indices):
    img = pool_images[idx]
    label = pool_labels[idx]

    # Predict
    img_input = tf.expand_dims(img, 0)
    pred = final_model.predict(img_input, verbose=0)

    # Decode Result
    true_idx = np.argmax(label)
    pred_idx = np.argmax(pred)
    true_name = class_names[true_idx]
    pred_name = class_names[pred_idx]
    conf = np.max(pred) * 100

    # Plot
    ax = plt.subplot(2, 5, i + 1)
    plt.imshow(img.numpy().astype("uint8"))

    col = "green" if true_idx == pred_idx else "red"

    # ‡∏à‡∏±‡∏î Format ‡∏ä‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡∏™‡∏±‡πâ‡∏ô‡∏•‡∏á‡∏´‡∏ô‡πà‡∏≠‡∏¢‡∏ñ‡πâ‡∏≤‡∏°‡∏±‡∏ô‡∏¢‡∏≤‡∏ß‡πÑ‡∏õ
    title = f"True: {true_name[:20]}...\nPred: {pred_name[:20]}...\n({conf:.1f}%)"
    plt.title(title, color=col, fontsize=10, fontweight='bold')
    plt.axis("off")

plt.tight_layout()
plt.show()

# ‡∏£‡∏ß‡∏°‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡∏à‡∏≤‡∏Å test_ds
y_true = []
y_pred = []

for images, labels in test_ds:
    preds = model.predict(images)
    y_true.extend(np.argmax(labels.numpy(), axis=1))
    y_pred.extend(np.argmax(preds, axis=1))

# ‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏õ‡πá‡∏ô numpy array
y_true = np.array(y_true)
y_pred = np.array(y_pred)

# Confusion Matrix
cm = confusion_matrix(y_true, y_pred)

plt.figure(figsize=(12, 10))
sns.heatmap(cm, cmap='Blues', xticklabels=False, yticklabels=False)
plt.title("Confusion Matrix")
plt.xlabel("Predicted")
plt.ylabel("True")
plt.show()

# Classification Report
print("üìÑ Classification Report:")
print(classification_report(y_true, y_pred, target_names=class_names))

def merge_and_plot_3_phases_from_log(h1_path, h2_path, h3_path):
    """‡∏£‡∏ß‡∏°‡∏£‡πà‡∏≤‡∏á‡∏Å‡∏£‡∏≤‡∏ü 3 ‡∏ä‡πà‡∏ß‡∏á (‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡∏ó‡∏±‡πâ‡∏á CSV Log ‡πÅ‡∏•‡∏∞ Pickle)"""

    # ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏ä‡πà‡∏ß‡∏¢‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏ü‡∏•‡πå (‡πÄ‡∏ä‡πá‡∏Ñ‡∏ß‡πà‡∏≤‡πÄ‡∏õ‡πá‡∏ô CSV ‡∏´‡∏£‡∏∑‡∏≠ Pickle)
    def load_file(path):
        if path.endswith('.csv'):
            # ‡∏≠‡πà‡∏≤‡∏ô CSV ‡πÅ‡∏•‡πâ‡∏ß‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏õ‡πá‡∏ô Dict ‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô History ‡∏õ‡∏Å‡∏ï‡∏¥
            return pd.read_csv(path).to_dict(orient='list')
        elif path.endswith('.pkl'):
            with open(path, 'rb') as f:
                return pickle.load(f)
        else:
            raise ValueError(f"Unknown file format: {path}")

    try:
        h1 = load_file(h1_path)
        h2 = load_file(h2_path)
        h3 = load_file(h3_path)
        print("‚úÖ Loaded all 3 history files successfully!")
    except Exception as e:
        print(f"‚ö†Ô∏è Error loading files: {e}")
        return

    # ‡∏£‡∏ß‡∏° History
    full_history = {}
    # ‡πÉ‡∏ä‡πâ keys ‡∏à‡∏≤‡∏Å h1 ‡πÄ‡∏õ‡πá‡∏ô‡∏´‡∏•‡∏±‡∏Å (accuracy, loss, etc.)
    for key in h1.keys():
        # ‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏° list ‡∏Ç‡∏≠‡∏á‡∏ó‡∏±‡πâ‡∏á 3 ‡πÄ‡∏ü‡∏™‡πÄ‡∏Ç‡πâ‡∏≤‡∏î‡πâ‡∏ß‡∏¢‡∏Å‡∏±‡∏ô
        # .get(key, []) ‡∏Å‡∏±‡∏ô‡πÄ‡∏´‡∏ô‡∏µ‡∏¢‡∏ß‡πÄ‡∏ú‡∏∑‡πà‡∏≠‡∏ö‡∏≤‡∏á‡πÑ‡∏ü‡∏•‡πå‡πÑ‡∏°‡πà‡∏°‡∏µ key ‡∏ô‡∏±‡πâ‡∏ô
        full_history[key] = h1.get(key, []) + h2.get(key, []) + h3.get(key, [])

    acc = full_history['accuracy']; val_acc = full_history['val_accuracy']
    loss = full_history['loss']; val_loss = full_history['val_loss']

    epochs = range(1, len(acc) + 1)

    # ‡∏à‡∏∏‡∏î‡∏ï‡∏±‡∏î‡πÅ‡∏ö‡πà‡∏á‡πÄ‡∏ü‡∏™ (‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏à‡∏≤‡∏Å‡∏Ñ‡∏ß‡∏≤‡∏°‡∏¢‡∏≤‡∏ß‡∏Ç‡∏≠‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏£‡∏¥‡∏á)
    split_1 = len(h1['accuracy']) # ‡∏à‡∏ö Phase 1
    split_2 = split_1 + len(h2['accuracy']) # ‡∏à‡∏ö Phase 2

    plt.figure(figsize=(15, 6))

    # --- Accuracy Plot ---
    plt.subplot(1, 2, 1)
    plt.plot(epochs, acc, label='Training Acc')
    plt.plot(epochs, val_acc, label='Validation Acc')

    # ‡πÄ‡∏™‡πâ‡∏ô‡πÅ‡∏ö‡πà‡∏á‡πÄ‡∏ü‡∏™
    plt.axvline(x=split_1, color='gray', linestyle='--', label='Start Fine-Tune 1')
    plt.axvline(x=split_2, color='green', linestyle=':', label='Start Fine-Tune 2 (Final)')

    # ‡∏´‡∏≤‡∏Ñ‡πà‡∏≤ Acc ‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î‡∏°‡∏≤‡πÇ‡∏ä‡∏ß‡πå
    max_acc = max(val_acc) if len(val_acc) > 0 else 0
    plt.title(f'ResNet50: 3-Stage Training (Final: {max_acc*100:.2f}%)')
    plt.legend(loc='lower right')
    plt.grid(True, alpha=0.3)

    # --- Loss Plot ---
    plt.subplot(1, 2, 2)
    plt.plot(epochs, loss, label='Training Loss')
    plt.plot(epochs, val_loss, label='Validation Loss')

    # ‡πÄ‡∏™‡πâ‡∏ô‡πÅ‡∏ö‡πà‡∏á‡πÄ‡∏ü‡∏™
    plt.axvline(x=split_1, color='gray', linestyle='--')
    plt.axvline(x=split_2, color='green', linestyle=':')

    plt.title('ResNet50: Loss Descent')
    plt.legend(loc='upper right')
    plt.grid(True, alpha=0.3)

    plt.show()

# ==========================================
# ‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡πÉ‡∏ä‡πâ (‡∏ä‡∏µ‡πâ‡πÑ‡∏õ‡∏ó‡∏µ‡πà‡πÑ‡∏ü‡∏•‡πå CSV ‡πÉ‡∏ô‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå logs)
# ==========================================
# ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö path ‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì‡πÉ‡∏´‡πâ‡∏î‡∏µ‡∏ß‡πà‡∏≤‡πÄ‡∏Å‡πá‡∏ö log ‡πÑ‡∏ß‡πâ‡∏ó‡∏µ‡πà‡πÑ‡∏´‡∏ô (‡∏õ‡∏Å‡∏ï‡∏¥‡∏ñ‡πâ‡∏≤‡πÉ‡∏ä‡πâ‡πÇ‡∏Ñ‡πâ‡∏î‡∏ú‡∏°‡∏à‡∏∞‡∏≠‡∏¢‡∏π‡πà‡∏ó‡∏µ‡πà logs/)
path1 = os.path.join(LOG_DIR, 'log_phase1.csv')
path2 = os.path.join(LOG_DIR, 'log_phase2.csv')
path3 = os.path.join(LOG_DIR, 'log_phase3.csv')

# ‡πÄ‡∏ä‡πá‡∏Ñ‡∏Å‡πà‡∏≠‡∏ô‡∏ß‡πà‡∏≤‡πÑ‡∏ü‡∏•‡πå‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà‡∏à‡∏£‡∏¥‡∏á‡πÑ‡∏´‡∏°
if os.path.exists(path1) and os.path.exists(path2) and os.path.exists(path3):
    merge_and_plot_3_phases_from_log(path1, path2, path3)
else:
    print("‚ùå ‡∏´‡∏≤‡πÑ‡∏ü‡∏•‡πå Log ‡πÑ‡∏°‡πà‡πÄ‡∏à‡∏≠‡∏Ñ‡∏£‡∏±‡∏ö ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡πÄ‡∏ä‡πá‡∏Ñ path ‡πÉ‡∏ô‡∏ï‡∏±‡∏ß‡πÅ‡∏õ‡∏£ path1, path2, path3")
    # ‡∏•‡∏≠‡∏á print path ‡∏≠‡∏≠‡∏Å‡∏°‡∏≤‡∏î‡∏π‡∏ñ‡πâ‡∏≤‡∏´‡∏≤‡πÑ‡∏°‡πà‡πÄ‡∏à‡∏≠
    print(f"Path1: {path1}")

# ‡∏´‡∏≤‡∏Ñ‡πà‡∏≤ FP ‡πÅ‡∏•‡∏∞ FN ‡∏£‡∏ß‡∏°‡∏Ç‡∏≠‡∏á‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏Ñ‡∏•‡∏≤‡∏™
cm_sum = np.sum(cm, axis=1, keepdims=True)
cm_norm = cm.astype('float') / cm_sum
np.fill_diagonal(cm_norm, 0)
confused_scores = cm_norm.sum(axis=1)

# ‡∏´‡∏≤ top 10 class ‡∏ó‡∏µ‡πà‡∏™‡∏±‡∏ö‡∏™‡∏ô‡∏°‡∏≤‡∏Å‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î
top_confused = np.argsort(confused_scores)[-10:][::-1]

print("üîç Top 10 Confusing Classes:")
for i in top_confused:
    print(f"- {class_names[i]}")

# ‡πÅ‡∏™‡∏î‡∏á matrix ‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏Ñ‡∏•‡∏≤‡∏™‡∏ó‡∏µ‡πà‡∏™‡∏±‡∏ö‡∏™‡∏ô‡∏°‡∏≤‡∏Å‡∏™‡∏∏‡∏î
plt.figure(figsize=(10, 8))
sns.heatmap(cm[top_confused][:, top_confused],
            xticklabels=[class_names[i] for i in top_confused],
            yticklabels=[class_names[i] for i in top_confused],
            cmap="Reds", annot=True, fmt="d")
plt.title("Top Confusing Classes (Subset of Confusion Matrix)")
plt.show()