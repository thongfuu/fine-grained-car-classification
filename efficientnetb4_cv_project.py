# -*- coding: utf-8 -*-
"""Final_‡∏Æ‡∏±‡∏•‡πÇ‡∏´‡∏•‡πÇ‡∏ö‡πã‡∏ß ‡πÇ‡∏Å‡πã‡∏°‡∏≤‡πÅ‡∏£‡πã‡∏ß‡∏ß_EfficientnetB4_FullFinetune_CV_project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1wLzzOSCpdBvHDInZTbQK0VCzqxssfCyt

# Import
"""

from google.colab import drive
drive.mount('/content/drive')

import zipfile
import os
import shutil
import scipy.io
from collections import Counter
import pandas as pd
import matplotlib.pyplot as plt
import random
from tqdm import tqdm
import numpy as np
import matplotlib.pyplot as plt
import itertools
import sklearn
from sklearn.metrics import classification_report, confusion_matrix
import seaborn as sns
import cv2
from tensorflow.keras import mixed_precision
import json
import pickle

from tensorflow import keras
import tensorflow as tf
from tensorflow.compat.v1 import ConfigProto, InteractiveSession
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.applications import EfficientNetB4
from tensorflow.keras.optimizers import AdamW
from tensorflow.keras import layers, regularizers, models
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint, CSVLogger
from tensorflow.keras.applications.efficientnet import preprocess_input

mixed_precision.set_global_policy('mixed_float16')

"""# ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏° Dataset"""

zip_path = "/content/drive/MyDrive/Datasets/stanford_car_dataset_by_classes.zip"
extract_path = "/content/Dataset"

# ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå‡∏ñ‡πâ‡∏≤‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏°‡∏µ
os.makedirs(extract_path, exist_ok=True)

# ‡πÅ‡∏ï‡∏Å‡πÑ‡∏ü‡∏•‡πå
with zipfile.ZipFile(zip_path, 'r') as zip_ref:
    zip_ref.extractall(extract_path)

DATASET_DIR = "/content/Dataset"

TRAIN_DIR = os.path.join(DATASET_DIR, "car_data", "car_data", "train")
TEST_DIR = os.path.join(DATASET_DIR, "car_data", "car_data", "test")

# ‡∏ô‡∏±‡∏ö‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏†‡∏≤‡∏û‡πÉ‡∏ô‡πÅ‡∏ï‡πà‡∏•‡∏∞‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå
train_counts = Counter()
for cls in os.listdir(TRAIN_DIR):
    cls_path = os.path.join(TRAIN_DIR, cls)
    if os.path.isdir(cls_path):
        n_images = len([f for f in os.listdir(cls_path) if f.lower().endswith(('.jpg', '.png', '.jpeg'))])
        train_counts[cls] = n_images

# ‡∏™‡∏£‡πâ‡∏≤‡∏á DataFrame
train_df = pd.DataFrame(train_counts.items(), columns=["Class", "Train Images"])
train_df = train_df.sort_values("Train Images", ascending=False)

# ‡πÅ‡∏™‡∏î‡∏á‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á
print("üìä ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏†‡∏≤‡∏û‡πÉ‡∏ô train:")
print(train_df.head(10))

# ‡∏û‡∏•‡πá‡∏≠‡∏ï‡∏Å‡∏£‡∏≤‡∏ü
plt.figure(figsize=(12, 5))
plt.bar(train_df["Class"], train_df["Train Images"], color="steelblue", label="Train")
plt.title("Distribution of images per class (Train set)")
plt.xlabel("Car Class")
plt.ylabel("Number of Images")
plt.xticks(rotation=90, fontsize=6)
plt.legend()
plt.tight_layout()
plt.show()

print("\n‡∏£‡∏ß‡∏°‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î:")
print(f"Train: {train_df['Train Images'].sum()} ‡∏£‡∏π‡∏õ")
print(f"‡∏£‡∏ß‡∏°‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î {len(train_df)} ‡∏Ñ‡∏•‡∏≤‡∏™")

ROOT_DIR = "/content/Dataset/car_data/car_data/train"
OUTPUT_DIR = "/content/FinalDataset"
TRAIN_OUT = os.path.join(OUTPUT_DIR, "train")
VAL_OUT = os.path.join(OUTPUT_DIR, "val")

TRAIN_RATIO = 0.75
VAL_RATIO = 0.25

random.seed(42)  # reproducible

# ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå‡∏õ‡∏•‡∏≤‡∏¢‡∏ó‡∏≤‡∏á
os.makedirs(TRAIN_OUT, exist_ok=True)
os.makedirs(VAL_OUT, exist_ok=True)

# ‡∏ß‡∏ô‡∏ó‡∏∏‡∏Å class
for class_name in tqdm(os.listdir(ROOT_DIR), desc="Splitting"):
    class_path = os.path.join(ROOT_DIR, class_name)
    if not os.path.isdir(class_path):
        continue

    images = [f for f in os.listdir(class_path) if f.lower().endswith(('.jpg', '.jpeg', '.png'))]
    if len(images) == 0:
        continue

    random.shuffle(images)

    # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏†‡∏≤‡∏û‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏ä‡∏∏‡∏î
    n_total = len(images)
    n_train = int(round(n_total * TRAIN_RATIO))
    n_val = n_total - n_train  # ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡∏£‡∏ß‡∏°‡∏Å‡∏±‡∏ô‡∏Ñ‡∏£‡∏ö‡∏û‡∏≠‡∏î‡∏µ

    train_imgs = images[:n_train]
    val_imgs   = images[n_train:]

    # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå class ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏ä‡∏∏‡∏î
    os.makedirs(os.path.join(TRAIN_OUT, class_name), exist_ok=True)
    os.makedirs(os.path.join(VAL_OUT, class_name), exist_ok=True)

    # copy ‡πÑ‡∏õ‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏ä‡∏∏‡∏î
    for img in train_imgs:
        shutil.copy2(os.path.join(class_path, img),
                     os.path.join(TRAIN_OUT, class_name, img))

    for img in val_imgs:
        shutil.copy2(os.path.join(class_path, img),
                     os.path.join(VAL_OUT, class_name, img))

def count_imgs(path):
    total = 0
    for cls in os.listdir(path):
        cls_dir = os.path.join(path, cls)
        if os.path.isdir(cls_dir):
            total += len(os.listdir(cls_dir))
    return total

print("Train images:", count_imgs(TRAIN_OUT))
print("Val images:  ", count_imgs(VAL_OUT))
print("Test images:  ", count_imgs(TEST_DIR))

"""# Train"""

print("GPU:", tf.config.list_physical_devices('GPU'))

# ‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô TensorFlow ‡∏à‡∏≠‡∏á VRAM ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î
gpus = tf.config.experimental.list_physical_devices('GPU')
for gpu in gpus:
    tf.config.experimental.set_memory_growth(gpu, True)

BASE_DIR = '/content/drive/MyDrive/Model/FinalEffnet_v2'
HISTORY_DIR = os.path.join(BASE_DIR, 'history')
MODEL_DIR = os.path.join(BASE_DIR, 'models')
LOG_DIR = os.path.join(BASE_DIR, 'logs')

for folder in [BASE_DIR, HISTORY_DIR, MODEL_DIR, LOG_DIR]:
    if not os.path.exists(folder):
        os.makedirs(folder)

TRAIN_DIR = "/content/FinalDataset/train"
VAL_DIR   = "/content/FinalDataset/val"
TEST_DIR  = "/content/Dataset/car_data/car_data/test"

IMG_SIZE = 380
BATCH_SIZE = 16
NUM_CLASSES = 196
SEED = 42

def set_global_seed(seed):
    os.environ['PYTHONHASHSEED'] = str(seed)
    random.seed(seed)
    np.random.seed(seed)
    tf.random.set_seed(seed)
    tf.keras.utils.set_random_seed(seed)

set_global_seed(SEED)

train_ds = tf.keras.utils.image_dataset_from_directory(
    TRAIN_DIR,
    image_size=(IMG_SIZE, IMG_SIZE),
    batch_size=BATCH_SIZE,
    label_mode='categorical',
    shuffle=True,
    seed=SEED
)

val_ds = tf.keras.utils.image_dataset_from_directory(
    VAL_DIR,
    image_size=(IMG_SIZE, IMG_SIZE),
    batch_size=BATCH_SIZE,
    label_mode='categorical',
    shuffle=False
)

test_ds = tf.keras.utils.image_dataset_from_directory(
    TEST_DIR,
    image_size=(IMG_SIZE, IMG_SIZE),
    batch_size=BATCH_SIZE,
    label_mode='categorical',
    shuffle=False
)

class_names = train_ds.class_names
with open(os.path.join(BASE_DIR, 'class_names.json'), 'w') as f:
    json.dump(train_ds.class_names, f)

train_ds = train_ds.prefetch(buffer_size=tf.data.AUTOTUNE)
val_ds = val_ds.prefetch(buffer_size=tf.data.AUTOTUNE)
test_ds = test_ds.prefetch(buffer_size=tf.data.AUTOTUNE)

"""## Augmentation"""

data_augmentation = keras.Sequential([
    layers.RandomFlip("horizontal"),
    layers.RandomRotation(0.2),
    layers.RandomZoom(0.2),
    layers.RandomContrast(0.2),
    layers.RandomTranslation(0.1, 0.1),
], name="data_augmentation")

"""## Model"""

base_model = tf.keras.applications.EfficientNetB4(
    include_top=False,
    weights="imagenet",
    input_shape=(IMG_SIZE, IMG_SIZE, 3)
)

base_model.trainable = False # ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô Freeze ‡πÑ‡∏ß‡πâ‡∏Å‡πà‡∏≠‡∏ô

inputs = tf.keras.Input(shape=(IMG_SIZE, IMG_SIZE, 3))
x = data_augmentation(inputs)

x = base_model(x, training=False)

x = tf.keras.layers.GlobalAveragePooling2D()(x)
x = tf.keras.layers.BatchNormalization()(x)
x = tf.keras.layers.Dropout(0.5)(x)

outputs = tf.keras.layers.Dense(NUM_CLASSES, activation="softmax", dtype='float32')(x)

model = tf.keras.Model(inputs, outputs)

def save_history_pickle(history, filename):
    path = os.path.join(HISTORY_DIR, filename)
    with open(path, 'wb') as f:
        pickle.dump(history.history, f)
    print(f"üíæ Pickle History saved: {path}")

optimizer = tf.keras.optimizers.AdamW(learning_rate=1e-3, weight_decay=1e-4)
loss_fn = tf.keras.losses.CategoricalCrossentropy(label_smoothing=0.2)

model.compile(optimizer=optimizer,
              loss=loss_fn,
              metrics=["accuracy"])

model.summary()

callbacks_phase1 = [
    EarlyStopping(patience=4, restore_best_weights=True, verbose=1),
    ModelCheckpoint(os.path.join(MODEL_DIR, 'effnet_phase1.keras'),
                    save_best_only=True, monitor='val_accuracy', mode='max'),
    CSVLogger(os.path.join(LOG_DIR, 'log_phase1.csv'), separator=',', append=True)
]

callbacks_phase2 = [
    EarlyStopping(patience=8, restore_best_weights=True, verbose=1),
    ModelCheckpoint(os.path.join(MODEL_DIR, 'effnet_best_final.keras'),
                    save_best_only=True, monitor='val_accuracy', mode='max'),
    ReduceLROnPlateau(factor=0.2, patience=3, min_lr=1e-7, verbose=1),
    CSVLogger(os.path.join(LOG_DIR, 'log_phase2.csv'), separator=',', append=True)
]

"""## Phase1"""

history1 = model.fit(
    train_ds,
    validation_data=val_ds,
    epochs=8,
    callbacks=callbacks_phase1
)

save_history_pickle(history1, 'history_phase1.pkl')

acc = history1.history['accuracy']
val_acc = history1.history['val_accuracy']

loss = history1.history['loss']
val_loss = history1.history['val_loss']

plt.figure(figsize=(8, 8))
plt.subplot(2, 1, 1)
plt.plot(acc, label='Training Accuracy')
plt.plot(val_acc, label='Validation Accuracy')
plt.legend(loc='lower right')
plt.ylabel('Accuracy')
plt.ylim([min(plt.ylim()),1])
plt.title('Training and Validation Accuracy')

plt.subplot(2, 1, 2)
plt.plot(loss, label='Training Loss')
plt.plot(val_loss, label='Validation Loss')
plt.legend(loc='upper right')
plt.ylabel('Cross Entropy')
plt.ylim([0,10.0])
plt.title('Training and Validation Loss')
plt.xlabel('epoch')
plt.show()

checkpoint_path = '/content/drive/MyDrive/Model/FinalEffnet_v2/effnet_phase1.keras'

# ‡πÇ‡∏´‡∏•‡∏î‡∏ô‡πâ‡∏≥‡∏´‡∏ô‡∏±‡∏Å (Weights) ‡∏Å‡∏•‡∏±‡∏ö‡∏°‡∏≤
print(f"‚è≥ Loading weights from {checkpoint_path}...")
try:
    model.load_weights(checkpoint_path)
    print("Load ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à!")
except Exception as e:
    print(f"Load ‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ: {e}")

"""## Finetune"""

base_model.trainable = True

optimizer_ft = tf.keras.optimizers.AdamW(learning_rate=5e-5, weight_decay=1e-4)

model.compile(optimizer=optimizer_ft,
              loss=loss_fn,
              metrics=["accuracy"])

model.summary()

history2 = model.fit(
    train_ds,
    validation_data=val_ds,
    epochs=40,
    callbacks=callbacks_phase2
)

save_history_pickle(history2, 'history_phase2.pkl')

latest_checkpoint = '/content/drive/MyDrive/Model/FinalEffnet_v2/models/effnet_best_final.keras'

print(f"‚è≥ Loading model from: {latest_checkpoint}")
if os.path.exists(latest_checkpoint):
    model = tf.keras.models.load_model(latest_checkpoint)
    print("‚úÖ Load ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à! ‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÄ‡∏ó‡∏£‡∏ô‡∏ï‡πà‡∏≠")
else:
    raise FileNotFoundError(f"‡πÑ‡∏°‡πà‡πÄ‡∏à‡∏≠‡πÑ‡∏ü‡∏•‡πå: {latest_checkpoint} ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡πÄ‡∏ä‡πá‡∏Ñ path ‡∏≠‡∏µ‡∏Å‡∏ó‡∏µ‡∏Ñ‡∏£‡∏±‡∏ö")

callbacks_resume = [
    EarlyStopping(patience=8, restore_best_weights=True, verbose=1),
    ModelCheckpoint(latest_checkpoint, # ‡πÄ‡∏ã‡∏ü‡∏ó‡∏±‡∏ö‡πÑ‡∏ü‡∏•‡πå‡πÄ‡∏î‡∏¥‡∏°‡πÑ‡∏õ‡πÄ‡∏•‡∏¢
                    save_best_only=True, monitor='val_accuracy', mode='max', verbose=1),
    ReduceLROnPlateau(factor=0.2, patience=3, min_lr=1e-7, verbose=1),
    CSVLogger('/content/drive/MyDrive/Model/FinalEffnet_v2/logs/log_phase2.csv', separator=',', append=True)
]

LAST_EPOCH = 13 # <--- ‡πÅ‡∏Å‡πâ‡πÄ‡∏•‡∏Ç‡∏ô‡∏µ‡πâ‡πÉ‡∏´‡πâ‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏à‡∏£‡∏¥‡∏á (‡∏Ñ‡∏£‡πà‡∏≤‡∏ß‡πÜ ‡∏Å‡πá‡πÑ‡∏î‡πâ)

# 5. ‡∏™‡∏±‡πà‡∏á‡πÄ‡∏ó‡∏£‡∏ô‡∏ï‡πà‡∏≠!
print(f"üöÄ Resuming training from Epoch {LAST_EPOCH+1}...")
history_resume = model.fit(
    train_ds,
    validation_data=val_ds,
    initial_epoch=LAST_EPOCH, # ‡∏ö‡∏≠‡∏Å‡πÉ‡∏´‡πâ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ô‡∏±‡∏ö‡∏ï‡πà‡∏≠‡∏à‡∏≤‡∏Å‡∏ô‡∏µ‡πâ
    epochs=40, # ‡πÄ‡∏õ‡πâ‡∏≤‡∏´‡∏°‡∏≤‡∏¢‡πÄ‡∏î‡∏¥‡∏°
    callbacks=callbacks_resume
)

latest_checkpoint = '/content/drive/MyDrive/Model/FinalEffnet_v2/models/effnet_best_final.keras'

print(f"‚è≥ Loading model from: {latest_checkpoint}")
if os.path.exists(latest_checkpoint):
    model = tf.keras.models.load_model(latest_checkpoint)
    print("‚úÖ Load ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à! ‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÄ‡∏ó‡∏£‡∏ô‡∏ï‡πà‡∏≠")
else:
    raise FileNotFoundError(f"‡πÑ‡∏°‡πà‡πÄ‡∏à‡∏≠‡πÑ‡∏ü‡∏•‡πå: {latest_checkpoint} ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡πÄ‡∏ä‡πá‡∏Ñ path ‡∏≠‡∏µ‡∏Å‡∏ó‡∏µ‡∏Ñ‡∏£‡∏±‡∏ö")

callbacks_resume = [
    EarlyStopping(patience=8, restore_best_weights=True, verbose=1),
    ModelCheckpoint(latest_checkpoint, # ‡πÄ‡∏ã‡∏ü‡∏ó‡∏±‡∏ö‡πÑ‡∏ü‡∏•‡πå‡πÄ‡∏î‡∏¥‡∏°‡πÑ‡∏õ‡πÄ‡∏•‡∏¢
                    save_best_only=True, monitor='val_accuracy', mode='max', verbose=1),
    ReduceLROnPlateau(factor=0.2, patience=3, min_lr=1e-7, verbose=1),
    CSVLogger('/content/drive/MyDrive/Model/FinalEffnet_v2/logs/log_phase2.csv', separator=',', append=True)
]

LAST_EPOCH = 19 # <--- ‡πÅ‡∏Å‡πâ‡πÄ‡∏•‡∏Ç‡∏ô‡∏µ‡πâ‡πÉ‡∏´‡πâ‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏à‡∏£‡∏¥‡∏á (‡∏Ñ‡∏£‡πà‡∏≤‡∏ß‡πÜ ‡∏Å‡πá‡πÑ‡∏î‡πâ)

# 5. ‡∏™‡∏±‡πà‡∏á‡πÄ‡∏ó‡∏£‡∏ô‡∏ï‡πà‡∏≠!
print(f"üöÄ Resuming training from Epoch {LAST_EPOCH+1}...")
history_resume = model.fit(
    train_ds,
    validation_data=val_ds,
    initial_epoch=LAST_EPOCH, # ‡∏ö‡∏≠‡∏Å‡πÉ‡∏´‡πâ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ô‡∏±‡∏ö‡∏ï‡πà‡∏≠‡∏à‡∏≤‡∏Å‡∏ô‡∏µ‡πâ
    epochs=40, # ‡πÄ‡∏õ‡πâ‡∏≤‡∏´‡∏°‡∏≤‡∏¢‡πÄ‡∏î‡∏¥‡∏°
    callbacks=callbacks_resume
)

"""## Model sanity check"""

# -------------------------------
# Check model output shape
# -------------------------------
x_sample = tf.random.normal((1, IMG_SIZE, IMG_SIZE, 3))
y_pred = model(x_sample)
print("‚úÖ Output shape:", y_pred.shape)
print("‚úÖ Sum of softmax probabilities:", tf.reduce_sum(y_pred).numpy())
assert y_pred.shape[-1] == NUM_CLASSES

train_ds_visual = tf.keras.utils.image_dataset_from_directory(
    TRAIN_DIR,
    image_size=(IMG_SIZE, IMG_SIZE),
    batch_size=BATCH_SIZE,
    label_mode='categorical',
    shuffle=True
)

class_names = train_ds_visual.class_names
images, labels = next(iter(train_ds_visual))

plt.figure(figsize=(10, 10))
for i in range(min(9, len(images))):
    ax = plt.subplot(3, 3, i + 1)
    plt.imshow(images[i].numpy().astype("uint8"))
    label_idx = np.argmax(labels[i].numpy())
    plt.title(class_names[label_idx])
    plt.axis("off")
plt.show()

# -------------------------------
# Predict batch ‡πÅ‡∏£‡∏Å (sanity check)
# -------------------------------
# ‡πÉ‡∏ä‡πâ train_ds ‡∏´‡∏£‡∏∑‡∏≠ test_ds preprocessed
images, labels = next(iter(test_ds))
preds = model.predict(images[:10])

for i in range(min(10, len(images))):
    plt.imshow(images[i].numpy().astype("uint8"))
    plt.axis("off")
    true_label = class_names[np.argmax(labels[i])]
    pred_label = class_names[np.argmax(preds[i])]
    confidence = np.max(preds[i])
    color = "green" if true_label == pred_label else "red"
    plt.title(f"Pred: {pred_label}\nTrue: {true_label}\nConf: {confidence:.2f}", color=color)
    plt.show()

# -------------------------------
# Check NaN / Inf weights
# -------------------------------
for layer in model.layers:
    for weight in layer.weights:
        w = weight.numpy()
        if np.isnan(w).any() or np.isinf(w).any():
            print(f"‚ö†Ô∏è ‡∏û‡∏ö NaN/Inf ‡πÉ‡∏ô layer: {layer.name}")
print("‚úÖ ‡πÑ‡∏°‡πà‡∏°‡∏µ NaN/Inf ‡πÉ‡∏ô‡πÇ‡∏°‡πÄ‡∏î‡∏•")

"""# Save Model"""

save_path = '/content/drive/MyDrive/Model/FinalEffnet_v2/models/effnet_final_complete.keras'

# 1. ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå‡∏Å‡πà‡∏≠‡∏ô‡πÄ‡∏™‡∏°‡∏≠ (‡∏Å‡∏±‡∏ô Error)
os.makedirs(os.path.dirname(save_path), exist_ok=True)

# 2. ‡πÄ‡∏ã‡∏ü‡πÇ‡∏°‡πÄ‡∏î‡∏•
model.save(save_path)
print(f"Model saved successfully at: {save_path}")

"""# Evaluate"""

# -------------------------------
# Evaluate ‡∏ö‡∏ô test set
# -------------------------------
test_loss, test_acc = model.evaluate(test_ds)
print(f"‚úÖ Test Accuracy: {test_acc:.4f}")
print(f"‚úÖ Test Loss: {test_loss:.4f}")

# -------------------------------
# Confusion Matrix + Classification Report
# -------------------------------
y_true = []
y_pred = []

for images, labels in test_ds:
    preds = model.predict(images)
    y_true.extend(np.argmax(labels.numpy(), axis=1))
    y_pred.extend(np.argmax(preds, axis=1))

y_true = np.array(y_true)
y_pred = np.array(y_pred)

cm = confusion_matrix(y_true, y_pred)

plt.figure(figsize=(12, 10))
sns.heatmap(cm, cmap='Blues', xticklabels=False, yticklabels=False)
plt.title("Confusion Matrix")
plt.xlabel("Predicted")
plt.ylabel("True")
plt.show()

print("üìÑ Classification Report:")
print(classification_report(y_true, y_pred, target_names=class_names))

def plot_history_from_logs(log1_path, log2_path):
    """‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Å‡∏£‡∏≤‡∏ü‡∏à‡∏≤‡∏Å‡πÑ‡∏ü‡∏•‡πå CSV Log"""

    # 1. ‡πÄ‡∏ä‡πá‡∏Ñ‡πÑ‡∏ü‡∏•‡πå‡∏Å‡πà‡∏≠‡∏ô
    if not os.path.exists(log1_path) or not os.path.exists(log2_path):
        print(f"‚ùå ‡∏´‡∏≤‡πÑ‡∏ü‡∏•‡πå Log ‡πÑ‡∏°‡πà‡πÄ‡∏à‡∏≠! ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡πÄ‡∏ä‡πá‡∏Ñ Path:\n1. {log1_path}\n2. {log2_path}")
        return

    # 2. ‡∏≠‡πà‡∏≤‡∏ô‡πÑ‡∏ü‡∏•‡πå CSV
    try:
        df1 = pd.read_csv(log1_path)
        df2 = pd.read_csv(log2_path)
        print("‚úÖ Loaded logs successfully!")
    except Exception as e:
        print(f"‚ö†Ô∏è Error reading CSV: {e}")
        return

    # 3. ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• (‡∏£‡∏ß‡∏°‡∏£‡πà‡∏≤‡∏á 2 ‡πÄ‡∏ü‡∏™)
    # ‡∏´‡∏°‡∏≤‡∏¢‡πÄ‡∏´‡∏ï‡∏∏: ‡∏ñ‡πâ‡∏≤‡πÉ‡∏ä‡πâ append=True ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏ô df2 ‡∏≠‡∏≤‡∏à‡∏à‡∏∞‡∏°‡∏µ‡∏´‡∏•‡∏≤‡∏¢‡∏ä‡πà‡∏ß‡∏á‡∏ï‡πà‡∏≠‡∏Å‡∏±‡∏ô ‡∏ã‡∏∂‡πà‡∏á‡∏î‡∏µ‡πÅ‡∏•‡πâ‡∏ß
    acc = df1['accuracy'].tolist() + df2['accuracy'].tolist()
    val_acc = df1['val_accuracy'].tolist() + df2['val_accuracy'].tolist()
    loss = df1['loss'].tolist() + df2['loss'].tolist()
    val_loss = df1['val_loss'].tolist() + df2['val_loss'].tolist()

    epochs = range(1, len(acc) + 1)
    split_at = len(df1) # ‡∏à‡∏∏‡∏î‡∏à‡∏ö Phase 1

    # 4. ‡∏û‡∏•‡πá‡∏≠‡∏ï‡∏Å‡∏£‡∏≤‡∏ü
    plt.figure(figsize=(15, 6))

    # --- Accuracy ---
    plt.subplot(1, 2, 1)
    plt.plot(epochs, acc, label='Training Acc')
    plt.plot(epochs, val_acc, label='Validation Acc')
    plt.axvline(x=split_at, color='gray', linestyle='--', label='Start Fine-Tuning')

    # ‡∏´‡∏≤‡∏Ñ‡πà‡∏≤ Max ‡∏°‡∏≤‡πÇ‡∏ä‡∏ß‡πå
    max_acc = max(val_acc)
    plt.title(f'EfficientNetB4 Training (Max Val Acc: {max_acc*100:.2f}%)')
    plt.legend(loc='lower right')
    plt.grid(True, alpha=0.3)

    # --- Loss ---
    plt.subplot(1, 2, 2)
    plt.plot(epochs, loss, label='Training Loss')
    plt.plot(epochs, val_loss, label='Validation Loss')
    plt.axvline(x=split_at, color='gray', linestyle='--', label='Start Fine-Tuning')

    plt.title('Training Loss')
    plt.legend(loc='upper right')
    plt.grid(True, alpha=0.3)

    plt.show()

# ‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô
path_log1 = os.path.join(LOG_DIR, 'log_phase1.csv')
path_log2 = os.path.join(LOG_DIR, 'log_phase2.csv')

plot_history_from_logs(path_log1, path_log2)

best_model_path = os.path.join(MODEL_DIR, 'effnet_best_final.keras')
print(f"‚è≥ Loading best model: {best_model_path}")
final_model = tf.keras.models.load_model(best_model_path)

pool_images = []
pool_labels = []

# ‡∏ß‡∏ô‡∏•‡∏π‡∏õ‡∏î‡∏∂‡∏á‡∏£‡∏π‡∏õ‡∏°‡∏≤‡∏™‡∏∞‡∏™‡∏° (‡∏î‡∏∂‡∏á‡∏°‡∏≤‡∏ó‡∏µ‡∏•‡∏∞ 1 ‡∏£‡∏π‡∏õ ‡∏à‡∏≤‡∏Å‡∏ó‡∏∏‡∏Å‡πÜ 3 Batch ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡∏Ñ‡∏•‡∏∞‡∏Å‡∏±‡∏ô)
for i, (batch_imgs, batch_lbls) in enumerate(test_ds):
    if i % 3 == 0: # ‡∏Å‡∏£‡∏∞‡πÇ‡∏î‡∏î‡∏Ç‡πâ‡∏≤‡∏° Batch ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡πÑ‡∏î‡πâ‡∏£‡∏ñ‡∏´‡∏•‡∏≤‡∏Å‡∏´‡∏•‡∏≤‡∏¢‡∏£‡∏∏‡πà‡∏ô
        # ‡∏™‡∏∏‡πà‡∏°‡∏´‡∏¢‡∏¥‡∏ö 1 ‡∏£‡∏π‡∏õ‡∏à‡∏≤‡∏Å‡πÉ‡∏ô Batch ‡∏ô‡∏µ‡πâ
        rand_idx = random.randint(0, len(batch_imgs) - 1)
        pool_images.append(batch_imgs[rand_idx])
        pool_labels.append(batch_lbls[rand_idx])

    # ‡∏û‡∏≠‡πÑ‡∏î‡πâ‡∏Ñ‡∏£‡∏ö‡∏™‡∏±‡∏Å 20-30 ‡∏£‡∏π‡∏õ‡∏Å‡πá‡∏û‡∏≠‡πÅ‡∏•‡πâ‡∏ß (‡πÄ‡∏¢‡∏≠‡∏∞‡πÑ‡∏õ‡πÄ‡∏î‡∏µ‡πã‡∏¢‡∏ß‡∏ä‡πâ‡∏≤)
    if len(pool_images) >= 20:
        break

# 3. ‡∏™‡∏∏‡πà‡∏°‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏°‡∏≤ 10 ‡∏£‡∏π‡∏õ‡∏à‡∏≤‡∏Å‡∏Å‡∏≠‡∏á‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏≤‡∏™‡∏∞‡∏™‡∏°‡∏°‡∏≤
indices = random.sample(range(len(pool_images)), min(10, len(pool_images)))

# 4. ‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•
plt.figure(figsize=(20, 8))

for i, idx in enumerate(indices):
    img = pool_images[idx]
    label = pool_labels[idx]

    # Predict
    img_input = tf.expand_dims(img, 0)
    pred = final_model.predict(img_input, verbose=0)

    # Decode Result
    true_idx = np.argmax(label)
    pred_idx = np.argmax(pred)
    true_name = class_names[true_idx]
    pred_name = class_names[pred_idx]
    conf = np.max(pred) * 100

    # Plot
    ax = plt.subplot(2, 5, i + 1)
    plt.imshow(img.numpy().astype("uint8"))

    col = "green" if true_idx == pred_idx else "red"

    # ‡∏à‡∏±‡∏î Format ‡∏ä‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡∏™‡∏±‡πâ‡∏ô‡∏•‡∏á‡∏´‡∏ô‡πà‡∏≠‡∏¢‡∏ñ‡πâ‡∏≤‡∏°‡∏±‡∏ô‡∏¢‡∏≤‡∏ß‡πÑ‡∏õ
    title = f"True: {true_name[:20]}...\nPred: {pred_name[:20]}...\n({conf:.1f}%)"
    plt.title(title, color=col, fontsize=10, fontweight='bold')
    plt.axis("off")

plt.tight_layout()
plt.show()

# -------------------------------
# Top 10 confusing classes
# -------------------------------
cm_sum = np.sum(cm, axis=1, keepdims=True)
cm_sum[cm_sum == 0] = 1  # ‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô‡∏´‡∏≤‡∏£‡∏î‡πâ‡∏ß‡∏¢ 0
cm_norm = cm.astype('float') / cm_sum
np.fill_diagonal(cm_norm, 0)
confused_scores = cm_norm.sum(axis=1)

top_confused = np.argsort(confused_scores)[-10:][::-1]

print("üîç Top 10 Confusing Classes:")
for i in top_confused:
    print(f"- {class_names[i]}")

plt.figure(figsize=(10, 8))
sns.heatmap(cm[top_confused][:, top_confused],
            xticklabels=[class_names[i] for i in top_confused],
            yticklabels=[class_names[i] for i in top_confused],
            cmap="Reds", annot=True, fmt="d")
plt.title("Top Confusing Classes (Subset of Confusion Matrix)")
plt.show()